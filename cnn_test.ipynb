{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models.MLP.model as MLP\n",
    "import models.CNN.model as CNN\n",
    "import importlib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "transform_MNIST = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda x: x.view(-1))\n",
    "        ])\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform_MNIST)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform_MNIST)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_cifar = transforms.Compose(\n",
    "    [transforms.ToTensor()])\n",
    "\n",
    "train_dataset_cifar = datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform_cifar)\n",
    "test_dataset_cifar = datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform_cifar)\n",
    "\n",
    "train_dataloader_cifar = DataLoader(train_dataset_cifar, batch_size=1)\n",
    "test_dataloader_cifar = DataLoader(test_dataset_cifar, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'models.CNN.model' from '/Users/genevievelarosedebilly/Desktop/Fall24/COMP396/biological-deep-learning/models/CNN/model.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(MLP)\n",
    "importlib.reload(CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softhebb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Demo single-file script to train a ConvNet on CIFAR10 using SoftHebb, an unsupervised, efficient and bio-plausible\n",
    "learning algorithm\n",
    "\"\"\"\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.utils import _pair\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torchvision\n",
    "\n",
    "\n",
    "class SoftHebbConv2d(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels: int,\n",
    "            out_channels: int,\n",
    "            kernel_size: int,\n",
    "            stride: int = 1,\n",
    "            padding: int = 0,\n",
    "            dilation: int = 1,\n",
    "            groups: int = 1,\n",
    "            t_invert: float = 12,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Simplified implementation of Conv2d learnt with SoftHebb; an unsupervised, efficient and bio-plausible\n",
    "        learning algorithm.\n",
    "        This simplified implementation omits certain configurable aspects, like using a bias, groups>1, etc. which can\n",
    "        be found in the full implementation in hebbconv.py\n",
    "        \"\"\"\n",
    "        super(SoftHebbConv2d, self).__init__()\n",
    "        assert groups == 1, \"Simple implementation does not support groups > 1.\"\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = _pair(kernel_size)\n",
    "        self.stride = _pair(stride)\n",
    "        self.dilation = _pair(dilation)\n",
    "        self.groups = groups\n",
    "        self.padding_mode = 'reflect'\n",
    "        self.F_padding = (padding, padding, padding, padding)\n",
    "        weight_range = 25 / math.sqrt((in_channels / groups) * kernel_size * kernel_size)\n",
    "        self.weight = nn.Parameter(weight_range * torch.randn((out_channels, in_channels // groups, *self.kernel_size)))\n",
    "        self.t_invert = torch.tensor(t_invert)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.pad(x, self.F_padding, self.padding_mode)  # pad input\n",
    "        # perform conv, obtain weighted input u \\in [B, OC, OH, OW]\n",
    "        weighted_input = F.conv2d(x, self.weight, None, self.stride, 0, self.dilation, self.groups)\n",
    "\n",
    "        if self.training:\n",
    "            # ===== find post-synaptic activations y = sign(u)*softmax(u, dim=C), s(u)=1 - 2*I[u==max(u,dim=C)] =====\n",
    "            # Post-synaptic activation, for plastic update, is weighted input passed through a softmax.\n",
    "            # Non-winning neurons (those not with the highest activation) receive the negated post-synaptic activation.\n",
    "            batch_size, out_channels, height_out, width_out = weighted_input.shape\n",
    "            # Flatten non-competing dimensions (B, OC, OH, OW) -> (OC, B*OH*OW)\n",
    "            flat_weighted_inputs = weighted_input.transpose(0, 1).reshape(out_channels, -1)\n",
    "            # Compute the winner neuron for each batch element and pixel\n",
    "            flat_softwta_activs = torch.softmax(self.t_invert * flat_weighted_inputs, dim=0)\n",
    "            flat_softwta_activs = - flat_softwta_activs  # Turn all postsynaptic activations into anti-Hebbian\n",
    "            win_neurons = torch.argmax(flat_weighted_inputs, dim=0)  # winning neuron for each pixel in each input\n",
    "            competing_idx = torch.arange(flat_weighted_inputs.size(1))  # indeces of all pixel-input elements\n",
    "            # Turn winner neurons' activations back to hebbian\n",
    "            flat_softwta_activs[win_neurons, competing_idx] = - flat_softwta_activs[win_neurons, competing_idx]\n",
    "            softwta_activs = flat_softwta_activs.view(out_channels, batch_size, height_out, width_out).transpose(0, 1)\n",
    "            # ===== compute plastic update Î”w = y*(x - u*w) = y*x - (y*u)*w =======================================\n",
    "            # Use Convolutions to apply the plastic update. Sweep over inputs with postynaptic activations.\n",
    "            # Each weighting of an input pixel & an activation pixel updates the kernel element that connected them in\n",
    "            # the forward pass.\n",
    "            yx = F.conv2d(\n",
    "                x.transpose(0, 1),  # (B, IC, IH, IW) -> (IC, B, IH, IW)\n",
    "                softwta_activs.transpose(0, 1),  # (B, OC, OH, OW) -> (OC, B, OH, OW)\n",
    "                padding=0,\n",
    "                stride=self.dilation,\n",
    "                dilation=self.stride,\n",
    "                groups=1\n",
    "            ).transpose(0, 1)  # (IC, OC, KH, KW) -> (OC, IC, KH, KW)\n",
    "\n",
    "            # sum over batch, output pixels: each kernel element will influence all batches and output pixels.\n",
    "            yu = torch.sum(torch.mul(softwta_activs, weighted_input), dim=(0, 2, 3))\n",
    "            delta_weight = yx - yu.view(-1, 1, 1, 1) * self.weight\n",
    "            delta_weight.div_(torch.abs(delta_weight).amax() + 1e-30)  # Scale [min/max , 1]\n",
    "            self.weight.grad = delta_weight  # store in grad to be used with common optimizers\n",
    "\n",
    "        return weighted_input\n",
    "\n",
    "\n",
    "class DeepSoftHebb(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepSoftHebb, self).__init__()\n",
    "        # block 1\n",
    "        self.bn1 = nn.BatchNorm2d(3, affine=False)\n",
    "        self.conv1 = SoftHebbConv2d(in_channels=3, out_channels=96, kernel_size=5, padding=2, t_invert=1,)\n",
    "        self.activ1 = Triangle(power=0.7)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=4, stride=2, padding=1)\n",
    "        # block 2\n",
    "        self.bn2 = nn.BatchNorm2d(96, affine=False)\n",
    "        self.conv2 = SoftHebbConv2d(in_channels=96, out_channels=384, kernel_size=3, padding=1, t_invert=0.65,)\n",
    "        self.activ2 = Triangle(power=1.4)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=4, stride=2, padding=1)\n",
    "        # block 3\n",
    "        self.bn3 = nn.BatchNorm2d(384, affine=False)\n",
    "        self.conv3 = SoftHebbConv2d(in_channels=384, out_channels=1536, kernel_size=3, padding=1, t_invert=0.25,)\n",
    "        self.activ3 = Triangle(power=1.)\n",
    "        self.pool3 = nn.AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        # block 4\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.classifier = nn.Linear(24576, 10)\n",
    "        self.classifier.weight.data = 0.11048543456039805 * torch.rand(10, 24576)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # block 1\n",
    "        out = self.pool1(self.activ1(self.conv1(self.bn1(x))))\n",
    "        # block 2\n",
    "        out = self.pool2(self.activ2(self.conv2(self.bn2(out))))\n",
    "        # block 3\n",
    "        out = self.pool3(self.activ3(self.conv3(self.bn3(out))))\n",
    "        # block 4\n",
    "        return self.classifier(self.dropout(self.flatten(out)))\n",
    "\n",
    "\n",
    "class Triangle(nn.Module):\n",
    "    def __init__(self, power: float = 1, inplace: bool = True):\n",
    "        super(Triangle, self).__init__()\n",
    "        self.inplace = inplace\n",
    "        self.power = power\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        input = input - torch.mean(input.data, axis=1, keepdims=True)\n",
    "        return F.relu(input, inplace=self.inplace) ** self.power\n",
    "\n",
    "\n",
    "class WeightNormDependentLR(optim.lr_scheduler._LRScheduler):\n",
    "    \"\"\"\n",
    "    Custom Learning Rate Scheduler for unsupervised training of SoftHebb Convolutional blocks.\n",
    "    Difference between current neuron norm and theoretical converged norm (=1) scales the initial lr.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, power_lr, last_epoch=-1, verbose=False):\n",
    "        self.optimizer = optimizer\n",
    "        self.initial_lr_groups = [group['lr'] for group in self.optimizer.param_groups]  # store initial lrs\n",
    "        self.power_lr = power_lr\n",
    "        super().__init__(optimizer, last_epoch, verbose)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
    "                          \"please use `get_last_lr()`.\", UserWarning)\n",
    "        new_lr = []\n",
    "        for i, group in enumerate(self.optimizer.param_groups):\n",
    "            for param in group['params']:\n",
    "                # difference between current neuron norm and theoretical converged norm (=1) scales the initial lr\n",
    "                # initial_lr * |neuron_norm - 1| ** 0.5\n",
    "                norm_diff = torch.abs(torch.linalg.norm(param.view(param.shape[0], -1), dim=1, ord=2) - 1) + 1e-10\n",
    "                new_lr.append(self.initial_lr_groups[i] * (norm_diff ** self.power_lr)[:, None, None, None])\n",
    "        return new_lr\n",
    "\n",
    "\n",
    "class TensorLRSGD(optim.SGD):\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step, using a non-scalar (tensor) learning rate.\n",
    "\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            weight_decay = group['weight_decay']\n",
    "            momentum = group['momentum']\n",
    "            dampening = group['dampening']\n",
    "            nesterov = group['nesterov']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                d_p = p.grad\n",
    "                if weight_decay != 0:\n",
    "                    d_p = d_p.add(p, alpha=weight_decay)\n",
    "                if momentum != 0:\n",
    "                    param_state = self.state[p]\n",
    "                    if 'momentum_buffer' not in param_state:\n",
    "                        buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                    else:\n",
    "                        buf = param_state['momentum_buffer']\n",
    "                        buf.mul_(momentum).add_(d_p, alpha=1 - dampening)\n",
    "                    if nesterov:\n",
    "                        d_p = d_p.add(buf, alpha=momentum)\n",
    "                    else:\n",
    "                        d_p = buf\n",
    "\n",
    "                p.add_(-group['lr'] * d_p)\n",
    "        return loss\n",
    "\n",
    "\n",
    "class CustomStepLR(StepLR):\n",
    "    \"\"\"\n",
    "    Custom Learning Rate schedule with step functions for supervised training of linear readout (classifier)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, nb_epochs):\n",
    "        threshold_ratios = [0.2, 0.35, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "        self.step_thresold = [int(nb_epochs * r) for r in threshold_ratios]\n",
    "        super().__init__(optimizer, -1, False)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch in self.step_thresold:\n",
    "            return [group['lr'] * 0.5\n",
    "                    for group in self.optimizer.param_groups]\n",
    "        return [group['lr'] for group in self.optimizer.param_groups]\n",
    "\n",
    "\n",
    "class FastCIFAR10(torchvision.datasets.CIFAR10):\n",
    "    \"\"\"\n",
    "    Improves performance of training on CIFAR10 by removing the PIL interface and pre-loading on the GPU (2-3x speedup).\n",
    "\n",
    "    Taken from https://github.com/y0ast/pytorch-snippets/tree/main/fast_mnist\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        device = kwargs.pop('device', \"cpu\")\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.data = torch.tensor(self.data, dtype=torch.float, device=device).div_(255)\n",
    "        self.data = torch.movedim(self.data, -1, 1)  # -> set dim to: (batch, channels, height, width)\n",
    "        self.targets = torch.tensor(self.targets, device=device)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        index : int\n",
    "            Index of the element to be returned\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            tuple: (image, target) where target is the index of the target class\n",
    "        \"\"\"\n",
    "        img = self.data[index]\n",
    "        target = self.targets[index]\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps')\n",
    "model = DeepSoftHebb()\n",
    "model.to(device)\n",
    "\n",
    "unsup_optimizer = TensorLRSGD([\n",
    "    {\"params\": model.conv1.parameters(), \"lr\": -0.08, },  # SGD does descent, so set lr to negative\n",
    "    {\"params\": model.conv2.parameters(), \"lr\": -0.005, },\n",
    "    {\"params\": model.conv3.parameters(), \"lr\": -0.01, },\n",
    "], lr=0)\n",
    "\n",
    "unsup_lr_scheduler = WeightNormDependentLR(unsup_optimizer, power_lr=0.5)\n",
    "\n",
    "sup_optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n",
    "sup_lr_scheduler = CustomStepLR(sup_optimizer, nb_epochs=50)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "trainset = FastCIFAR10('./data', train=True, download=True)\n",
    "unsup_trainloader = torch.utils.data.DataLoader(trainset, batch_size=10, shuffle=True, num_workers=0)\n",
    "sup_trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=0)\n",
    "\n",
    "testset = FastCIFAR10('./data', train=False)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1000, shuffle=False, num_workers=0)\n",
    "\n",
    " # Unsupervised training with SoftHebb\n",
    "running_loss = 0.0\n",
    "for data in tqdm(unsup_trainloader):\n",
    "    inputs, _ = data\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    # zero the parameter gradients\n",
    "    unsup_optimizer.zero_grad()\n",
    "\n",
    "    # forward + update computation\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "\n",
    "    # optimize\n",
    "    unsup_optimizer.step()\n",
    "    unsup_lr_scheduler.step()\n",
    "\n",
    "# Supervised training of classifier\n",
    "# set requires grad false and eval mode for all modules but classifier\n",
    "unsup_optimizer.zero_grad()\n",
    "model.conv1.requires_grad = False\n",
    "model.conv2.requires_grad = False\n",
    "model.conv3.requires_grad = False\n",
    "model.conv1.eval()\n",
    "model.conv2.eval()\n",
    "model.conv3.eval()\n",
    "model.bn1.eval()\n",
    "model.bn2.eval()\n",
    "model.bn3.eval()\n",
    "for epoch in range(1):\n",
    "    model.classifier.train()\n",
    "    model.dropout.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in tqdm(sup_trainloader):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        sup_optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        sup_optimizer.step()\n",
    "\n",
    "        # compute training statistics\n",
    "        running_loss += loss.item()\n",
    "        if epoch % 10 == 0 or epoch == 49:\n",
    "            total += labels.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    sup_lr_scheduler.step()\n",
    "\n",
    "    print(f'Accuracy of the network on the train images: {100 * correct // total} %')\n",
    "    print(f'[{epoch + 1}] loss: {running_loss / total:.3f}')\n",
    "\n",
    "    # on the test set\n",
    "    model.eval()\n",
    "    running_loss = 0.\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')\n",
    "    print(f'test loss: {running_loss / total:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp, lab = next(iter(testloader))\n",
    "image = inp[0].permute(1,2,0)\n",
    "label = lab[0]\n",
    "input1 = inp[0].unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x300073b10>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = model.pool1(model.activ1(model.conv1(model.bn1(input1))))\n",
    "h2 = model.pool2(model.activ2(model.conv2(model.bn2(h1))))\n",
    "h3 = model.pool3(model.activ3(model.conv3(model.bn3(h2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(h1.shape)\n",
    "print(h2.shape)\n",
    "print(h3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view(h1):\n",
    "    ac1 = h1.squeeze().detach().to('cpu')\n",
    "    print(ac1.shape)\n",
    "    nbe = int(math.ceil(math.sqrt(ac1.size(0))))\n",
    "\n",
    "\n",
    "    fig, axes = plt.subplots(nbe, nbe, figsize=(20,20))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i > ac1.size(0)-1:\n",
    "            break\n",
    "        ax.imshow(ac1[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view(h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view(h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view(h3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hebbian CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topdown = False\n",
    "model = CNN.CNN_Model_SoftHeb_from_config(inputshape=(3, 32, 32), kernels=[3, 3, 3], channels=[32, 128, 512], strides=[2, 2, 1],\n",
    "                                padding=[0, 0, 0], lambd=4, lr=0.005, gamma=0.99, epsilon=0.01, b=1,\n",
    "                                rho=0.001, nbclasses=10, topdown=topdown, device=\"cpu\", eta=0.1,\n",
    "                                learningrule=CNN.Learning.SoftHebb,\n",
    "                                weightscaling=CNN.WeightScale.WeightNormalization,\n",
    "                                outputlayerrule=CNN.ClassifierLearning.SoftHebb, triangle=True, whiten_input=False, inhibition=CNN.Inhibition.Lateral)\n",
    "mymodelCNN = CNN.CNN_Experiment(epoch=1, mymodel=model, dataloader=train_dataloader_cifar, testloader= test_dataloader_cifar,\n",
    "                                        dataset='CIFAR10',\n",
    "                                    nclasses=10, imgtype=CNN.ImageType.RGB, device='cpu', traintopdown=topdown, testtopdown=topdown)\n",
    "# print(CNN.CNN_Baseline_test(mymodel=mymodelCNN, data_loader=test_dataloader, imgtype=CNN.ImageType.RGB,\n",
    "#                             topdown=topdown))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_device = torch.device('cpu')\n",
    "input1 = input1.to(cpu_device)\n",
    "label = label.to(cpu_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "u1, o1 = mymodelCNN.layers['CNNLayer1'].forward(input1, label, False)\n",
    "u2, o2 = mymodelCNN.layers['CNNLayer2'].forward(o1, None, False)\n",
    "u3, o3 = mymodelCNN.layers['CNNLayer3'].forward(o2, None, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view(o3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view(o2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view(o1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
