{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# PART 1: Imports\n",
    "##############################################################################\n",
    "\n",
    "# Built-in imports\n",
    "import logging\n",
    "import argparse\n",
    "import os\n",
    "from operator import itemgetter\n",
    "from pathlib import Path\n",
    "\n",
    "# Pytorch imports\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.distributed as dist\n",
    "from torch import nn\n",
    "from torch.nn.functional import one_hot\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Custom defined model imports\n",
    "from models.hebbian_network import HebbianNetwork # Model import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# PART 2: Parse arguments for the experiment\n",
    "##############################################################################\n",
    "\n",
    "# Simulate the command line arguments\n",
    "args_dict = {\n",
    "    '--is_training': True,\n",
    "    '--data_name': 'MNIST',\n",
    "    '--train_data': 'data/mnist/train-images.idx3-ubyte',\n",
    "    '--train_label': 'data/mnist/train-labels.idx1-ubyte',\n",
    "    '--test_data': 'data/mnist/t10k-images.idx3-ubyte',\n",
    "    '--test_label': 'data/mnist/t10k-labels.idx1-ubyte',\n",
    "    '--train_filename': 'data/mnist/mnist_train.csv',\n",
    "    '--test_filename': 'data/mnist/mnist_test.csv',\n",
    "    '--input_dim': 784,\n",
    "    '--heb_dim': 64,\n",
    "    '--output_dim': 10,\n",
    "    '--heb_lr': 0.05,\n",
    "    '--heb_lamb': 15,\n",
    "    '--heb_gam': 0.99,\n",
    "    '--cla_lr': 0.05,\n",
    "    '--cla_lamb': 1,\n",
    "    '--eps': 0.01,\n",
    "    '--epochs': 3,\n",
    "    '--test-epochs': 1,\n",
    "    '--dropout': 0.2,\n",
    "    '--lr': 0.01,\n",
    "    '--lr-step-size': 1000,\n",
    "    '--gamma': 1,\n",
    "    '--batch-size': 1,\n",
    "    '--device-id': 'cpu'\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a list of arguments\n",
    "args_list = []\n",
    "for k, v in args_dict.items():\n",
    "    args_list.append(k)\n",
    "    args_list.append(str(v))\n",
    "\n",
    "# Argument parser\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Basic configurations.\n",
    "parser.add_argument('--is_training', type=bool, default=True, help='status')\n",
    "parser.add_argument('--data_name', type=str, default=\"MNIST\")\n",
    "\n",
    "# Data Factory\n",
    "parser.add_argument('--train_data', type=str, default=\"data/mnist/train-images.idx3-ubyte\")\n",
    "parser.add_argument('--train_label', type=str, default=\"data/mnist/train-labels.idx1-ubyte\")\n",
    "parser.add_argument('--test_data', type=str, default=\"data/mnist/t10k-images.idx3-ubyte\")\n",
    "parser.add_argument('--test_label', type=str, default=\"data/mnist/t10k-labels.idx1-ubyte\")\n",
    "\n",
    "# CSV files generated\n",
    "parser.add_argument('--train_filename', type=str, default=\"data/mnist/mnist_train.csv\")\n",
    "parser.add_argument('--test_filename', type=str, default=\"data/mnist/mnist_test.csv\")\n",
    "\n",
    "# Dimension of each layer\n",
    "parser.add_argument('--input_dim', type=int, default=784)\n",
    "parser.add_argument('--heb_dim', type=int, default=64)\n",
    "parser.add_argument('--output_dim', type=int, default=10)\n",
    "\n",
    "# Hebbian layer hyperparameters\n",
    "parser.add_argument('--heb_lr', type=float, default=0.005)\n",
    "parser.add_argument('--heb_lamb', type=float, default=15)\n",
    "parser.add_argument('--heb_gam', type=float, default=0.99)\n",
    "\n",
    "# Classification layer hyperparameters\n",
    "parser.add_argument('--cla_lr', type=float, default=0.005)\n",
    "parser.add_argument('--cla_lamb', type=float, default=1)\n",
    "\n",
    "# Shared hyperparameters\n",
    "parser.add_argument('--eps', type=float, default=10e-5)\n",
    "\n",
    "# ---------------------------------------\n",
    "\n",
    "# The number of times to loop over the whole dataset\n",
    "parser.add_argument(\"--epochs\", type=int, default=3)\n",
    "\n",
    "# Testing model performance on a test every \"test-epochs\" epochs\n",
    "parser.add_argument(\"--test-epochs\", type=int, default=1)\n",
    "\n",
    "# A model training regularisation technique to reduce over-fitting\n",
    "parser.add_argument(\"--dropout\", type=float, default=0.2)\n",
    "\n",
    "# This example demonstrates a StepLR learning rate scheduler. Different schedulers will require different hyper-parameters.\n",
    "parser.add_argument(\"--lr\", type=float, default=0.01)\n",
    "parser.add_argument(\"--lr-step-size\", type=int, default=1000)\n",
    "parser.add_argument(\"--gamma\", type=float, default=1)\n",
    "\n",
    "# ---------------------------------------\n",
    "parser.add_argument(\"--batch-size\", type=int, default=1)\n",
    "parser.add_argument(\"--device-id\", type=str, default='cpu')\n",
    "\n",
    "# ---------------------------------------\n",
    "args = parser.parse_args(args_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# PART 3: Helper functions\n",
    "##############################################################################\n",
    "def get_optimizer(model):\n",
    "    optimizer = optim.Adam(model.get_module(\"Hebbian Layer\").parameters(), 0.001)\n",
    "    return optimizer\n",
    "\n",
    "def get_loss_function():\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    return loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# PART 4: Training\n",
    "##############################################################################\n",
    "\"\"\"\n",
    "Method defining how a single training epoch works\n",
    "@param\n",
    "    model (models.Network) = the network that is being trained\n",
    "    train_data_loader (torch.DataLoader) = dataloader with the training data\n",
    "    test_data_loader (torch.DataLoader) = dataloader with testing data\n",
    "    args (argparse.ArgumentParser) = arguments that were passed to the function\n",
    "@return\n",
    "    ___ (void) = no returns\n",
    "\"\"\"\n",
    "def train_loop(model, train_data_loader, test_data_loader, args, optimizer):\n",
    "    # Set the model to training mode - important for layers with different training / inference behaviour\n",
    "    model.train()\n",
    "\n",
    "    # Loop through training batches\n",
    "    for inputs, targets in train_data_loader:\n",
    "        # Move input and targets to device\n",
    "        inputs, targets = inputs.to(args.device_id).float(), one_hot(targets, 10).squeeze().to(args.device_id).float()\n",
    "        \n",
    "        # Forward pass\n",
    "        model(inputs, clamped_output=targets)\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# PART 5: Testing\n",
    "##############################################################################\n",
    "\"\"\"\n",
    "Method that test the model at certain epochs during the training process\n",
    "@param\n",
    "    model (models.Network) = model to be trained\n",
    "    train_data_loder (torch.DataLoader) = dataloader containing the training dataset\n",
    "    test_data_loader (torch.DataLoader) = dataloader containing the testing dataset\n",
    "    args (argparse.ArgumentParser) = arguments that are pased from the command shell\n",
    "@return\n",
    "    ___ (void) = no returns\n",
    "\"\"\"\n",
    "def test_loop(model, train_data_loader, test_data_loader, args, epoch):\n",
    "    # Set the model to evaluation mode - important for layers with different training / inference behaviour\n",
    "    model.eval()\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode \n",
    "    #  also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Loop thorugh testing batches\n",
    "        for inputs, targets in test_data_loader:\n",
    "            # Move input and targets to device\n",
    "            inputs, targets = inputs.to(args.device_id), targets.to(args.device_id)\n",
    "            \n",
    "            # Inference\n",
    "            predictions = model(inputs)\n",
    "            \n",
    "            # Performance metrics logging\n",
    "            correct = (predictions.argmax(1) == targets).type(torch.float).sum() \n",
    "            \n",
    "            # Degubbing purposes\n",
    "            debug = logging.getLogger(\"Debug Log\")\n",
    "            debug.info(f\"Prediciton/Actual: {predictions.argmax(1).item()}/{targets.item()}.\")\n",
    "\n",
    "            # Test logging\n",
    "            test = logging.getLogger(\"Test Log\")\n",
    "            test.info(f'Epoch Number: {epoch} || Test Accuracy: {correct/len(test_data_loader)}') \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Method to test the model on the entire testing dataset\n",
    "@param\n",
    "    model (models.Network) = ML model to be tested\n",
    "    test_dataset (torch.DataSet) = testing dataset that model will be tested on\n",
    "@return\n",
    "    correct/len(data_loader) (floattt) = accuracy of the model\n",
    "\"\"\"\n",
    "def model_test(model, test_data):\n",
    "    model.eval()\n",
    "\n",
    "    data_loader = DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "    correct = 0\n",
    "    total = len(test_data)\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs.to(args.device_id), targets.to(args.device_id)\n",
    "            predictions = model(inputs)\n",
    "            correct += (predictions.argmax(1) == targets).type(torch.float).sum()\n",
    "\n",
    "    return correct/total\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Method to test the model on the entire testing dataset\n",
    "@param\n",
    "    model (models.Network) = ML model to be tested\n",
    "    test_data_loader (torch.DataLoader) = testing data loader that model will be tested on\n",
    "@return\n",
    "    correct/len(data_loader) (float) = accuracy of the model\n",
    "\"\"\"\n",
    "def testing(model, test_dataset):\n",
    "    model.eval()\n",
    "\n",
    "    test_data_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in test_data_loader.items():\n",
    "        outputs = torch.argmax(model(inputs, None))\n",
    "        if outputs.item() == labels.item():\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# PART 6: Main Function\n",
    "##############################################################################\n",
    "\"\"\"\n",
    "Method describing the main part of the code -> how experiment will be ran\n",
    "@param\n",
    "    args (argparse.ArgumentParser) = arguments passed to the main function\n",
    "@return\n",
    "    ___ (void) = no returns\n",
    "\"\"\"\n",
    "def main(args):\n",
    "    # ===========================================\n",
    "    # Distributed Training Configuration\n",
    "    # ===========================================\n",
    "    args.device_id = 'cpu'\n",
    "    torch.device(args.device_id)\n",
    "\n",
    "\n",
    "    # ===========================================\n",
    "    # Set up model\n",
    "    # ===========================================\n",
    "    model = HebbianNetwork(args).float()\n",
    "    model = model.to(args.device_id)\n",
    "\n",
    "\n",
    "    # ===========================================\n",
    "    # Set up datasets for training and testing purposes\n",
    "    # ===========================================\n",
    "    \n",
    "    # Training dataset\n",
    "    train_data_set = model.get_module(\"Input Layer\").setup_train_data()\n",
    "    train_data_loader = DataLoader(train_data_set, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "    # Testing dataset\n",
    "    test_data_set = model.get_module(\"Input Layer\").setup_test_data()\n",
    "    test_data_loader = DataLoader(test_data_set, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "    # ===========================================\n",
    "    # Training and testing process\n",
    "    # ===========================================\n",
    "\n",
    "    #\n",
    "    optimizer = get_optimizer(model)\n",
    "    loss_function = get_loss_function()\n",
    "   \n",
    "    # Loops through each epoch from current epoch to total number of epochs\n",
    "    for epoch in range(0, args.epochs): \n",
    "        train_loop(\n",
    "            model, \n",
    "            train_data_loader, \n",
    "            test_data_loader, \n",
    "            args,\n",
    "            optimizer\n",
    "        )\n",
    "\n",
    "    model.visualize_weights(folder_path)\n",
    "    accuracy_1 = model_test(model, test_data_set) # Final test after entire training\n",
    "    accuracy_2 = testing(model, test_data_loader)\n",
    "    param = logging.getLogger(\"Parameter Log\")\n",
    "    param.info(f\"Accuracy 1 of model after training for {args.epochs} epochs: {accuracy_1}\")\n",
    "    param.info(f\"Accuracy 2 of model after training for {args.epochs} epochs: {accuracy_2}\")\n",
    "\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# PART 7: What code will be ran when file is ran\n",
    "##############################################################################\n",
    "# Helper function\n",
    "\"\"\"\n",
    "Method to create a logger to log information\n",
    "@param\n",
    "    name (str) = name of logger\n",
    "    file (str) - path to file\n",
    "    level (logging.Level) = level of the log\n",
    "    format (logging.Formatter) = format of the log\n",
    "@return\n",
    "    logger (logging.Logger) = a logger\n",
    "\"\"\"\n",
    "def configure_logger(name, file, level=logging.INFO, format=logging.Formatter('%(asctime)s || %(message)s')):\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level)\n",
    "    handler = logging.FileHandler(file)\n",
    "    handler.setLevel(level)\n",
    "    handler.setFormatter(format)\n",
    "    logger.addHandler(handler)\n",
    "    logger.propagate = False\n",
    "    return logger\n",
    "\n",
    "\n",
    "# Actual code that will be ran\n",
    "args = parser.parse_args(args_list)\n",
    "\n",
    "# Create folder in results to store training and testing results for this experiment\n",
    "exp_num = 'cpu-1'\n",
    "folder_path = f\"results/experiment-{exp_num}\"\n",
    "log_result_path = folder_path + \"/testing.log\"\n",
    "log_param_path = folder_path + \"/parameters.log\"\n",
    "log_debug_path = folder_path + \"/debug.log\"\n",
    "log_print_path = folder_path + \"/prints.log\"\n",
    "log_basic_path = folder_path + \"/basic.log\"\n",
    "log_format = logging.Formatter('%(asctime)s || %(message)s')\n",
    "log_level = logging.INFO\n",
    "\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    print(f\"Experiment {exp_num} result folder created successfully.\")\n",
    "else:\n",
    "    print(f\"Experiment {exp_num} result folder already exists.\")\n",
    "\n",
    "# Create logs\n",
    "print_log = configure_logger(\"Print Log\", log_print_path, log_level, log_format) # Replace print statements (for debugging purposes)\n",
    "test_log = configure_logger(\"Test Log\", log_result_path, log_level, log_format) # Test accuracy\n",
    "param_log = configure_logger(\"Parameter Log\", log_param_path, log_level, log_format) # Experiment parameters\n",
    "debug_log = configure_logger(\"Debug Log\", log_debug_path, log_level, log_format) # Debugging stuff\n",
    "\n",
    "# Logging training parameters\n",
    "if os.path.getsize(log_param_path) == 0:\n",
    "    param_log.info(f\"Input Dimension: {args.input_dim}\")\n",
    "    param_log.info(f\"Hebbian Layer Dimension: {args.heb_dim}\")\n",
    "    param_log.info(f\"Outout Dimension: {args.output_dim}\")\n",
    "    param_log.info(f\"Hebbian Layer Learning Rate: {args.heb_lr}\")\n",
    "    param_log.info(f\"Hebbian Layer Lambda: {args.heb_lamb}\")\n",
    "    param_log.info(f\"Hebbian Layer Gamma: {args.heb_gam}\")\n",
    "    param_log.info(f\"Classification Layer Learning Rate: {args.cla_lr}\")\n",
    "    param_log.info(f\"Classification Layer Lambda: {args.cla_lamb}\")\n",
    "    param_log.info(f\"Epsilon: {args.eps}\")\n",
    "    param_log.info(f\"Number of Epochs: {args.epochs}\")\n",
    "\n",
    "# Run experiment\n",
    "main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
