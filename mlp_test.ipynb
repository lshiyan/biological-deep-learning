{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhaoyizhe/miniconda3/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/zhaoyizhe/miniconda3/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <CFED5F8E-EC3F-36FD-AAA3-2C6C7F8D3DD9> /Users/zhaoyizhe/miniconda3/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Expected in:     <539E3895-EEBD-3044-801F-9B618F314545> /Users/zhaoyizhe/miniconda3/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import models.MLP.model as MLP\n",
    "import models.MLP.experiments as experiments\n",
    "import models.CNN.model as CNN\n",
    "import models.MLP.layers as layers_file\n",
    "import importlib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from models.MLP.generate_configs import generate_mlp_config_files\n",
    "import json\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import models.learning as L\n",
    "\n",
    "transform_MNIST = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda x: x.view(-1))\n",
    "        ])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=False, transform=transform_MNIST)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=False, transform=transform_MNIST)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'models.learning' from '/Users/zhaoyizhe/Documents/GitHub/biological-deep-learning/models/learning.py'>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(MLP)\n",
    "importlib.reload(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_mlp_config_files(hsizes=[64], lambds=[5], wlrs=[0.01], blrs=[0.01], llrs=[0.01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ConfigsMLP/config0.json\", \"r\") as file:\n",
    "    config = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c43392cbd9747ea8c4f9a2cdd116891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m MLP\u001b[38;5;241m.\u001b[39mNewMLPBaseline_Model(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhsize\u001b[39m\u001b[38;5;124m\"\u001b[39m], config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlambd\u001b[39m\u001b[38;5;124m\"\u001b[39m], config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw_lr\u001b[39m\u001b[38;5;124m\"\u001b[39m], config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb_lr\u001b[39m\u001b[38;5;124m\"\u001b[39m], config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml_lr\u001b[39m\u001b[38;5;124m\"\u001b[39m], nclasses\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Set greedytrain = true for layer wise training \u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m experiments\u001b[38;5;241m.\u001b[39mSoftMLPBaseline_Experiment(\u001b[38;5;241m1\u001b[39m, model, train_dataloader, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMNIST\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m10\u001b[39m, device, greedytrain\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/GitHub/biological-deep-learning/models/MLP/experiments.py:67\u001b[0m, in \u001b[0;36mSoftMLPBaseline_Experiment\u001b[0;34m(epoch, mymodel, dataloader, dataset, nclasses, device, greedytrain)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m tqdm(dataloader):\n\u001b[1;32m     66\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m---> 67\u001b[0m     mymodel\u001b[38;5;241m.\u001b[39mforward(inputs, oneHotEncode(labels, nclasses, device))\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer_name, layer \u001b[38;5;129;01min\u001b[39;00m mymodel\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(layer, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlamb\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/GitHub/biological-deep-learning/models/MLP/model.py:138\u001b[0m, in \u001b[0;36mSoftNeuralNet.forward\u001b[0;34m(self, x, clamped)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, clamped\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m--> 138\u001b[0m         x \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mforward(x, target\u001b[38;5;241m=\u001b[39mclamped)\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Documents/GitHub/biological-deep-learning/models/MLP/layers.py:121\u001b[0m, in \u001b[0;36mSoftHebbLayer.forward\u001b[0;34m(self, x, target)\u001b[0m\n\u001b[1;32m    119\u001b[0m inference_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference(x)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m--> 121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearn_weights(inference_output, target\u001b[38;5;241m=\u001b[39mtarget)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inference_output\u001b[38;5;241m.\u001b[39my\n",
      "File \u001b[0;32m~/Documents/GitHub/biological-deep-learning/models/MLP/layers.py:99\u001b[0m, in \u001b[0;36mSoftHebbLayer.learn_weights\u001b[0;34m(self, inference_output, target)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn_weights\u001b[39m(\u001b[38;5;28mself\u001b[39m, inference_output, target\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     98\u001b[0m     supervised \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearningrule \u001b[38;5;241m==\u001b[39m LearningRule\u001b[38;5;241m.\u001b[39mSoftHebbOutputContrastive\n\u001b[0;32m---> 99\u001b[0m     delta_w \u001b[38;5;241m=\u001b[39m L\u001b[38;5;241m.\u001b[39mupdate_softhebb_w(inference_output\u001b[38;5;241m.\u001b[39my, inference_output\u001b[38;5;241m.\u001b[39mxn, inference_output\u001b[38;5;241m.\u001b[39ma,\n\u001b[1;32m    100\u001b[0m                                   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minhibition, inference_output\u001b[38;5;241m.\u001b[39mu, target\u001b[38;5;241m=\u001b[39mtarget,\n\u001b[1;32m    101\u001b[0m                                   supervised\u001b[38;5;241m=\u001b[39msupervised, weight_growth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_growth)\n\u001b[1;32m    102\u001b[0m     delta_b \u001b[38;5;241m=\u001b[39m L\u001b[38;5;241m.\u001b[39mupdate_softhebb_b(inference_output\u001b[38;5;241m.\u001b[39my, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogprior, target\u001b[38;5;241m=\u001b[39mtarget, supervised\u001b[38;5;241m=\u001b[39msupervised)\n\u001b[1;32m    103\u001b[0m     delta_l \u001b[38;5;241m=\u001b[39m L\u001b[38;5;241m.\u001b[39mupdate_softhebb_lamb(inference_output\u001b[38;5;241m.\u001b[39my, inference_output\u001b[38;5;241m.\u001b[39ma, inhibition\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minhibition,\n\u001b[1;32m    104\u001b[0m                                      lamb\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlamb\u001b[38;5;241m.\u001b[39mitem(), in_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_dim, target\u001b[38;5;241m=\u001b[39mtarget,\n\u001b[1;32m    105\u001b[0m                                      supervised\u001b[38;5;241m=\u001b[39msupervised)\n",
      "File \u001b[0;32m~/Documents/GitHub/biological-deep-learning/models/learning.py:82\u001b[0m, in \u001b[0;36mupdate_softhebb_w\u001b[0;34m(y, normed_x, a, weights, inhibition, u, target, supervised, weight_growth)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_softhebb_w\u001b[39m(y, normed_x, a, weights, inhibition: Inhibition, u\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, target\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     81\u001b[0m                       supervised\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, weight_growth: WeightGrowth \u001b[38;5;241m=\u001b[39m WeightGrowth\u001b[38;5;241m.\u001b[39mDefault):\n\u001b[0;32m---> 82\u001b[0m     weight_norms \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(weights, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     83\u001b[0m     normed_weights \u001b[38;5;241m=\u001b[39m weights \u001b[38;5;241m/\u001b[39m (weight_norms \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-9\u001b[39m)\n\u001b[1;32m     84\u001b[0m     batch_dim, out_dim \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/functional.py:1615\u001b[0m, in \u001b[0;36mnorm\u001b[0;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[1;32m   1613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfro\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dim, (\u001b[38;5;28mint\u001b[39m, torch\u001b[38;5;241m.\u001b[39mSymInt)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dim) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m   1614\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1615\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mvector_norm(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m2\u001b[39m, _dim, keepdim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   1616\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1617\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mvector_norm(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m2\u001b[39m, _dim, keepdim, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = MLP.NewMLPBaseline_Model(config[\"hsize\"], config[\"lambd\"], config[\"w_lr\"], config[\"b_lr\"], config[\"l_lr\"], nclasses=10, device=device)\n",
    "# Set greedytrain = true for layer wise training \n",
    "model = experiments.SoftMLPBaseline_Experiment(1, model, train_dataloader, 'MNIST', 10, device, greedytrain=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing the model...\n",
      "Accuracy on the test set: 9.80%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTesting the model...\")\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():  # Disable gradient calculation for testing\n",
    "    for data in test_dataloader:  # Loop over the test set\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model.forward(inputs)  # Forward pass\n",
    "        _, predicted = torch.max(outputs.data, 1)  # Get the index of the max log-probability\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Accuracy on the test set: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
