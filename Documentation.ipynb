{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # Creates argument parser object\n",
    "    parser = argparse.ArgumentParser(description='Biological deep learning')\n",
    "    \n",
    "    # Adds arguments to the parser and sets their default values\n",
    "    parser.add_argument('--is_training', type=int, default=1, help='status')\n",
    "    parser.add_argument('--data_root', type=str, default=\"data/mnist/\")\n",
    "    parser.add_argument('--train_data_filename', type=str, default=\"mnist_train.csv\")\n",
    "    parser.add_argument('--test_data_filename', type=str, default=\"mnist_test.csv\")\n",
    "    parser.add_argument('--learning_rate', type=int, default=0.001)\n",
    "    parser.add_argument('--num_epochs', type=int, default=100)\n",
    "    \n",
    "    # Parse arguments given to the parser\n",
    "    args=parser.parse_args()\n",
    "    \n",
    "    if args.is_training:\n",
    "        print(\"training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data_loader.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\"\n",
    "Class to setup datasets (seperating images and labels) for both training and testing purposes\n",
    "\"\"\"\n",
    "class Image_Data_Set(Dataset):\n",
    "    \"\"\"\n",
    "    Contructor method\n",
    "    @param\n",
    "        train (bool) = is the dataset in training or testing\n",
    "        name (str) = name of data set\n",
    "    @attr.\n",
    "        flag (str) = a string indicating wether the data is used for training or testing\n",
    "        data_frame (list-like object) = dataset for training/testing\n",
    "        labels (torch.Tensor) = labels of dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, train=True, name='MNIST'):\n",
    "        self.__name = name\n",
    "        self.__flag = 'train' if train else 'test'\n",
    "        self.__data_frame = None\n",
    "        self.__labels = None\n",
    "\n",
    "    \"\"\"\n",
    "    Set up the data frame and labels with the defined data set\n",
    "    @param\n",
    "        data_set (str) = string defining path to .csv file of data set\n",
    "    \"\"\"\n",
    "    def setup_data(self, data_set):\n",
    "        self.data_frame=pd.read_csv(data_set, header=None)\n",
    "        self.labels=torch.tensor(self.dataframe[0].values)\n",
    "        self.data_frame=torch.tensor(self.data_frame.drop(self.data_frame.columns[0], axis=1).values, dtype=torch.float)\n",
    "        self.data_frame/=255\n",
    "    \n",
    "    # Normalize number(s) assuming max value = 255\n",
    "    def normalize(self, row):\n",
    "        row=row/255\n",
    "    \n",
    "    # Get length of data frame    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    # Attritube getter functions\n",
    "    def get_flag(self):\n",
    "        return self.__flag\n",
    "    \n",
    "    def get_name(self):\n",
    "        return self.__name\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        label = self.labels[idx]\n",
    "        features = self.dataframe[idx]\n",
    "        \n",
    "        return features, label\n",
    "\n",
    "\n",
    "\n",
    "# TODO: Write this part of the code somewhere else\n",
    "if __name__==\"__main__\":\n",
    "    #mnist=MNIST_set(None)\n",
    "    #fashion_MNIST_set(None)\n",
    "    mnist = Image_Data_Set(name=\"MNIST\")\n",
    "    fashion_mnist = Image_Data_Set(name=\"Fashion_MNIST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mnist_factory.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\"\"\"\n",
    "Converts image file and labels into .csv file\n",
    "@param \n",
    "    img_file (str) = relative path to image file\n",
    "    label_file (str) = relative path to label file\n",
    "    out_file (str) = relative path to out file\n",
    "    data_size (int) = number of data inputs to read\n",
    "\"\"\"\n",
    "\n",
    "def convert(img_file, label_file, out_file, data_size):\n",
    "    # Get absolute path of all the necessary files\n",
    "    project_root = os.getcwd()\n",
    "    img_file = os.path.join(project_root, img_file)\n",
    "    label_file = os.path.join(project_root, label_file)\n",
    "    out_file = os.path.join(project_root, out_file)\n",
    "\n",
    "    # Open all necessary files\n",
    "    imgs = open(img_file, \"rb\")\n",
    "    out = open(out_file, \"w\")\n",
    "    labels = open(label_file, \"rb\")\n",
    "    \n",
    "    # Skip start of file because???\n",
    "    # TODO: why skip bytes?\n",
    "    imgs.read(16)\n",
    "    labels.read(8)\n",
    "    \n",
    "    # Create a 2D list of images where each image is a 1D list where the first element is the label\n",
    "    img_size = 28*28\n",
    "    images = []\n",
    "\n",
    "    for i in range(data_size):\n",
    "        image = [ord(labels.read(1))]\n",
    "        for j in range(img_size):\n",
    "            image.append(ord(imgs.read(1)))\n",
    "        images.append(image)\n",
    "\n",
    "    # Convert each image from 1D list to a comma seperated str and write it into out file\n",
    "    for image in images:\n",
    "        out.write(\",\".join(str(pix) for pix in image)+\"\\n\")\n",
    "    \n",
    "    # Close files\n",
    "    imgs.close()\n",
    "    out.close()\n",
    "    labels.close()\n",
    "\n",
    "# TODO: These lines should be written somewhere else\n",
    "convert(\"data/mnist/train-images.idx3-ubyte\", \"data/mnist/train-labels.idx1-ubyte\",\n",
    "        \"data/mnist/mnist_train.csv\", 60000)\n",
    "convert(\"data/mnist/t10k-images.idx3-ubyte\", \"data/mnist/t10k-labels.idx1-ubyte\",\n",
    "        \"data/mnist/mnist_test.csv\", 10000)\n",
    "\n",
    "convert(\"data/fashion_mnist/train-images.idx3-ubyte\", \"data/fashion_mnist/train-labels.idx1-ubyte\",\n",
    "        \"data/fashion_mnist/fashion-mnist_train.csv\", 60000)\n",
    "convert(\"data/fashion_mnist/t10k-images.idx3-ubyte\", \"data/fashion_mnist/t10k-labels.idx1-ubyte\",\n",
    "        \"data/fashion_mnist/fashion-mnist_test.csv\", 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hebbian_layer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import outer\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Hebbian learning layer that implements lateral inhibition in output. Not trained through supervision.\n",
    "\n",
    "\"\"\"\n",
    "HebbianLayer class is a module for neural network layers that use Hebbian learning.\n",
    "\"\"\"\n",
    "class HebbianLayer (nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    Constructor method for a hebbian layer\n",
    "    @param\n",
    "        input_dimension (int) = the number of input features for the layer\n",
    "        output_dimension (int) = the number of output features/neurons for the layer\n",
    "        classifier (bool) = a boolean that indicates whether this layer will act as a classifier\n",
    "        lamb (float) = hyperparameter controlling strength of lateral inhibition\n",
    "        heb_lr (float) = learning rate for hebbian learning updates\n",
    "        gamma (float) = hyperparameter used for exponential moving average, which influences the learning updates\n",
    "        eps (float) = a very small hyperparameter value used to prevent division by zero \n",
    "    \"\"\"\n",
    "    def __init__(self, input_dimension, output_dimension, classifier, lamb=2, heb_lr=0.001, gamma=0.99, eps=10e-5):\n",
    "        super (HebbianLayer, self).__init__()\n",
    "\n",
    "        # This block of code initialize everything specified by constructor parameter\n",
    "        self.input_dimension=input_dimension\n",
    "        self.output_dimension=output_dimension\n",
    "        self.lamb=lamb\n",
    "        self.alpha = heb_lr\n",
    "        self.isClassifier=classifier\n",
    "        self.scheduler=None\n",
    "        self.eps=eps\n",
    "        self.exponential_average=torch.zeros(self.output_dimension)\n",
    "        self.gamma=gamma\n",
    "        \n",
    "        #This block of code initialize all pytorch neural network components u\n",
    "        self.fc=nn.Linear(self.input_dimension, self.output_dimension, bias=True)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "        self.softplus=nn.Softplus()\n",
    "        self.tanh=nn.Tanh()\n",
    "        self.softmax=nn.Softmax()\n",
    "\n",
    "        self.itensors=self.createITensors()\n",
    "        \n",
    "        #This block of code initialize the weights \n",
    "        for param in self.fc.parameters():\n",
    "            # Employed uniform weight initialization\n",
    "            param=torch.nn.init.uniform_(param, a=0.0, b=1.0)\n",
    "            param.requires_grad_(False)\n",
    "        \n",
    "        \n",
    "    def setScheduler(self, scheduler):\n",
    "        self.scheduler=scheduler\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    Method creates identity tensors for Sanger's rule computation (which ensures orthogonality and prevent redundancy among neurons)\n",
    "    @return \n",
    "        the itensors\n",
    "    \"\"\"\n",
    "    def createITensors(self):\n",
    "        itensors=torch.zeros(self.output_dimension, self.output_dimension, self.output_dimension, dtype=torch.float)\n",
    "        for i in range(0, self.output_dimension):\n",
    "            identity = torch.eye(i+1)\n",
    "            padded_identity = torch.nn.functional.pad(identity, (0, self.output_dimension - i-1, 0, \n",
    "                                                                 self.output_dimension - i-1))\n",
    "            itensors[i]=padded_identity\n",
    "        return itensors\n",
    "    \n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Method calculates the lateral inhibition. \n",
    "    The mathematical equation calculated with this function is the following:  ' h_mu = (h_mu)^(lambda)/ max on i (h_mu_i)^(lambda) '\n",
    "    @return \n",
    "        the answer to the equation specified above\n",
    "    \"\"\"\n",
    "    def inhibition(self, x):\n",
    "        x=self.relu(x) \n",
    "        max_ele=torch.max(x).item()\n",
    "        x=torch.pow(x, self.lamb)\n",
    "        x/=abs(max_ele)**self.lamb\n",
    "        return x\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    Method performs weight updates for a Hebbian learning-based classifier, \n",
    "    This method incorporates elements of both supervised and unsupervised learning depending on the provided parameters.\n",
    "    @param\n",
    "        input (vector) = the input feature vector to the layer\n",
    "        output (vector) = the raw output logits from the layer before applying activation\n",
    "        clamped_output (vector) = an OPTIONAL target output vector -- Only applicable if the layer is classifier layer\n",
    "        train (bool) = a flag indicating whether the network is in training mode\n",
    "                            - If not in training mode, weights are not updated\n",
    "    \"\"\"\n",
    "    def updateWeightsHebbianClassifier(self, input, output, clamped_output=None, train=1):\n",
    "\n",
    "        if train:\n",
    "            # This block of code clone and detach the input,output tensors from the graph to avoid backprop effects\n",
    "            # Then the tensors are squeezed to remove singleton dimensions, which siplifies tensor for computation\n",
    "            u=output.clone().detach().squeeze()\n",
    "            x=input.clone().detach().squeeze()\n",
    "\n",
    "            # Transform the logits into a probability distribution over all output classes\n",
    "            y=torch.softmax(u, dim=0)\n",
    "\n",
    "\n",
    "            # This block of code is our weight update mechanism - This is separated into 2 parts\n",
    "            # This is the first part, where we find the scaled adjustments needed for part 2\n",
    "            A=None\n",
    "            # If clamped_output IS PROVIDED, then I have a supervised learning scenario\n",
    "            if clamped_output != None:\n",
    "                # This outer product of the error and input vector indicates how much and in which direction the weights should be adjusted based on the error.\n",
    "                outer_prod=torch.outer(clamped_output-y,x)\n",
    "\n",
    "                # Perform element-wise multiplication of the logits and their corresponding probabilities\n",
    "                # This is used to scale the updates.\n",
    "                u_times_y=torch.mul(u,y)\n",
    "\n",
    "                # Adjusts the computed outer product by subtracting a scaled version of the current weights. \n",
    "                # This adjustment is to regulate the updates, ensuring that they don't grow too large and contribute to overfitting or instability.\n",
    "                A=outer_prod-self.fc.weight*(u_times_y.unsqueeze(1))\n",
    "\n",
    "            # Unsupervised learning scenario is when clamped_output is NOT PROVIDED\n",
    "            else:\n",
    "                # Computes the outer product of the output probabilities and the input vector, \n",
    "                # This is a typical Hebbian update rule suggesting that weights should increase for co-occurring input-output pairs.\n",
    "                A=torch.outer(y,x)\n",
    "\n",
    "            \n",
    "            # This is the second part of our weight update mechanism, where we apply the scaled adjustments found in part 1\n",
    "\n",
    "            A=self.fc.weight+self.alpha*A # Updates the weights by adding the computed adjustments scaled by the learning rate\n",
    "            weight_maxes=torch.max(A, dim=1).values\n",
    "            self.fc.weight=nn.Parameter(A/weight_maxes.unsqueeze(1), requires_grad=False) # Normalizes each row of the weight matrix\n",
    "            self.fc.weight[:, 0] = 0 # This sets the first element of each weight vector to zero. THIS IS SO THAT THE REST OF THE NEURONS START LEARNING\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Method implements a variant of Hebbian learning that is enhanced by Sanger's rule for orthogonal feature extraction. \n",
    "    This method ensures that as new features are learned, they do not replicate the information captured by previously adjusted weights, leading to a more diverse and representative set of learned features. \n",
    "    The method also includes an element of adaptivity through the use of an exponential moving average. \n",
    "    This method is integral to unsupervised learning paradigms where the model learns to represent input data without explicit target outputs.\n",
    "\n",
    "    In other words, the following equations are calculated:\n",
    "        1. Employed Sanger's Rule, deltaW_(ij)=alpha*x_j*y_i-alpha*y_i*sum(k=1 to i) (w_(kj)*y_k)\n",
    "        2. Calculated outer product of input and output and adds it to matrix.\n",
    "\n",
    "    @param\n",
    "        input (vector) = the input feature vector to the layer\n",
    "        output (vector) = the raw output logits from the layer before applying activation\n",
    "        clamped_output (vector) = DID NOT USE IN THIS FUNCTION... SO REDUNDANT CODE\n",
    "        train (bool) = a flag indicating whether the network is in training mode\n",
    "                            - If not in training mode, weights are not updated\n",
    "    \"\"\"\n",
    "    def updateWeightsHebbian(self, input, output, clamped_output=None, train=1):\n",
    "        if train:\n",
    "            x=torch.tensor(input.clone().detach(), requires_grad=False, dtype=torch.float).squeeze()\n",
    "            y=torch.tensor(output.clone().detach(), requires_grad=False, dtype=torch.float).squeeze()\n",
    "\n",
    "            # This matrix represents the product of post-activation outputs and input features**, which is the core of the Hebbian learning update (recall assocaitive learning)\n",
    "            outer_prod=torch.tensor(outer(y, x))\n",
    "\n",
    "            # This block of code implements sanger's rule\n",
    "            initial_weight=torch.transpose(self.fc.weight.clone().detach(), 0,1)\n",
    "            # This adjust weights based on their contributions to earlier outputs, effectively implementing Sanger's rule which ensures that features learned by earlier neurons are orthogonal to those learned by subsequent neurons.\n",
    "            A=torch.einsum('jk, lkm, m -> lj', initial_weight, self.itensors, y)\n",
    "            # Modifies the adjustment matrix by scaling it with the output activations, aligning the updates with the level of neuron activation.\n",
    "            A=A*(y.unsqueeze(1))\n",
    "\n",
    "            # This block of code performs weight update\n",
    "            # Calculates the weight delta by scaling the difference between the outer product and the Sanger's rule adjustments by the learning rate\n",
    "            delta_weight=self.alpha*(outer_prod-A)\n",
    "            # Applies the weight update\n",
    "            self.fc.weight=nn.Parameter(torch.add(self.fc.weight, delta_weight), requires_grad=False)\n",
    "\n",
    "            # This line of code updates an exponential moving average of the outputs, which will be used in weight decay\n",
    "            self.exponential_average=torch.add(self.gamma*self.exponential_average,(1-self.gamma)*y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def updateBias(self, output, train=1):\n",
    "        if train:\n",
    "            y=output.clone().detach().squeeze()\n",
    "            exponential_bias=torch.exp(-1*self.fc.bias)\n",
    "            A=torch.mul(exponential_bias, y)-1\n",
    "            A=self.fc.bias+self.alpha*A\n",
    "            bias_maxes=torch.max(A, dim=0).values\n",
    "            self.fc.bias=nn.Parameter(A/bias_maxes.item(), requires_grad=False)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    This method aims to:\n",
    "        1. decay overused weights and \n",
    "        2. boost underused ones based on their activity, measured by the exponential moving average of the outputs.\n",
    "    This approach helps in maintaining a balanced network, preventing any single neuron from dominating the learning process excessively.\n",
    "    \"\"\"\n",
    "    def weightDecay(self):\n",
    "\n",
    "        # This block of code uses exponential moving average \n",
    "        average=torch.mean(self.exponential_average).item() # This average provides a baseline to compare individual neuron activities against.\n",
    "        # This line creates a ratio of each neuron's activity relative to the average activity. \n",
    "            # Values greater than 1 indicate neurons that are more active than average, while \n",
    "            # values less than 1 indicate less active neurons.\n",
    "        A=self.exponential_average/average \n",
    "\n",
    "        # This block of code calculates the growth factors\n",
    "        # Calculates a growth factor for more active neurons. \n",
    "            # This factor scales down as the activity (**`A`**) increases beyond the average\n",
    "        growth_factor_positive=self.eps*self.tanh(-self.eps*(A-1))+1 \n",
    "    # growth_factor_positive = self.eps * 2 * self.tanh(-self.eps * 10000000 * (A - 1)) + 1\n",
    "        # For underused neurons, this factor is the reciprocal of the positive growth factor, \n",
    "            # meaning it will scale up weights that are less active than average.\n",
    "        growth_factor_negative=torch.reciprocal(growth_factor_positive)\n",
    "\n",
    "        # This block of code applies the growth factors and updates weights\n",
    "        positive_weights=torch.where(self.fc.weight>0, self.fc.weight, 0.0)\n",
    "        negative_weights=torch.where(self.fc.weight<0, self.fc.weight, 0.0)\n",
    "        positive_weights=positive_weights*growth_factor_positive.unsqueeze(1)\n",
    "        negative_weights=negative_weights*growth_factor_negative.unsqueeze(1)\n",
    "        self.fc.weight=nn.Parameter(torch.add(positive_weights, negative_weights), requires_grad=False)\n",
    "        if (self.fc.weight.isnan().any()):\n",
    "            print(\"NAN WEIGHT\")\n",
    "    \n",
    "\n",
    "    #Feed forward\n",
    "    \"\"\"\n",
    "    This method aims to define:\n",
    "        1. how input data is processed through the layer and \n",
    "        2. how various other functions—such as weight updates and activations—are integrated during the forward pass\n",
    "    @param\n",
    "        x (vector) = input vector to the layer\n",
    "        clamped_output (vector)=  An optional tensor that can be used to provide target outputs for supervised learning scenarios.\n",
    "        train (bool) = a flag indicating whether the layer is in training mode\n",
    "    \"\"\"\n",
    "        \n",
    "    def forward(self, x, clamped_output=None, train=1):\n",
    "        input=x.clone()\n",
    "\n",
    "        # IF the current layer is a classifier\n",
    "        if self.isClassifier:\n",
    "            x=self.fc(x) #  Applies the linear transformation defined by the layer's weights and biases to the input tensor. \n",
    "            self.updateWeightsHebbianClassifier(input, x, clamped_output=clamped_output, train=train) # adjusts the weights based on the output and the optional clamped output\n",
    "            #self.updateBias(x, train=train)\n",
    "            x=self.softmax(x)\n",
    "            return x\n",
    "        # IF the current layer is NOT A CLASSIFIER\n",
    "        else:\n",
    "            x=self.fc(x)\n",
    "            x=self.inhibition(x) # Applies an inhibition function to the transformed outputs. This function modulates the activations to implement lateral inhibition, enhancing feature contrast and reducing redundancy.\n",
    "            self.updateWeightsHebbian(input, x, train) # Performs the weight update using a general Hebbian learning rule, which might include aspects of Sanger's rule for feature decorrelation.\n",
    "            #self.updateBias(x, train=train)\n",
    "            self.weightDecay() \n",
    "            return x\n",
    "    \n",
    "    #Counts the number of active feature selectors (above a certain cutoff beta).\n",
    "    def activeClassifierWeights(self, beta):\n",
    "        weights=self.fc.weight\n",
    "        active=torch.where(weights>beta, weights, 0.0)\n",
    "        return active.nonzero().size(0)\n",
    "    \n",
    "    #Creates heatmap of randomly chosen feature selectors.\n",
    "    def visualizeWeights(self, classifier=0):\n",
    "        weight = self.fc.weight\n",
    "        if classifier:\n",
    "            fig, axes = plt.subplots(2, 5, figsize=(16, 8))\n",
    "            for ele in range(10):  \n",
    "                random_feature_selector = weight[ele]\n",
    "                heatmap = random_feature_selector.view(int(math.sqrt(self.fc.weight.size(1))),\n",
    "                                                        int(math.sqrt(self.fc.weight.size(1))))\n",
    "                ax = axes[ele // 5, ele % 5]\n",
    "                im = ax.imshow(heatmap, cmap='hot', interpolation='nearest')\n",
    "                fig.colorbar(im, ax=ax)\n",
    "                ax.set_title(f'Weight {ele}')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            fig, axes = plt.subplots(8, 8, figsize=(16, 16))\n",
    "            for ele in range(self.output_dimension): \n",
    "                random_feature_selector = weight[ele]\n",
    "                heatmap = random_feature_selector.view(int(math.sqrt(self.fc.weight.size(1))),\n",
    "                                                        int(math.sqrt(self.fc.weight.size(1))))\n",
    "                ax = axes[ele // 8, ele % 8]\n",
    "                im = ax.imshow(heatmap, cmap='hot', interpolation='nearest')\n",
    "                fig.colorbar(im, ax=ax)\n",
    "                ax.set_title(f'Weight {ele}')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mlp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from data.data_loader import MNIST_set, fashion_MNIST_set\n",
    "from models.hebbian_network import HebbianNetwork\n",
    "from layers.scheduler import Scheduler\n",
    "\n",
    "\"\"\"\n",
    "Class to create and run experiments using model\n",
    "\"\"\"\n",
    "class MLPExperiment():\n",
    "    \"\"\"\n",
    "    Constructor moethod\n",
    "    @param\n",
    "        args () =\n",
    "        input_dimension (int) = number of input data\n",
    "        hidden_layer_dimension (int) = number of neurons in hidden layer\n",
    "        output_dimension (int) = number of output neurons\n",
    "        lamb (float) = hyperparameter for lateral inhibition\n",
    "        heb_lr (float) = hebbian layer learning rate\n",
    "        grad_lr (float) = gradiant descent learning rate\n",
    "        num_epochs (int) = number of epochs\n",
    "        gamma (float) = decay factor for learning rate\n",
    "        eps (float) = small number to avoid 0 division\n",
    "    \"\"\"\n",
    "    def __init__(self, args, input_dimension, hidden_layer_dimension, output_dimension, lamb=1, heb_lr=1, grad_lr=0.001, num_epochs=3, gamma=0, eps=10e-5):\n",
    "        self.model = HebbianNetwork(input_dimension, hidden_layer_dimension, output_dimension, heb_lr=heb_lr, lamb=lamb, eps=eps) # TODO: For some reason my hebbian network is not processing batches together.\n",
    "        self.args = args\n",
    "        self.num_epochs = num_epochs\n",
    "        self.grad_lr = grad_lr \n",
    "        self.heb_lr = heb_lr\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    # Return Adam optimizer\n",
    "    def optimizer(self):\n",
    "        optimizer = optim.Adam(self.model.parameters(), self.grad_lr)\n",
    "        return optimizer\n",
    "    \n",
    "    # Set the scheduler for hebbian layer.\n",
    "    def set_hebbian_scheduler(self):\n",
    "        scheduler = Scheduler(self.heb_lr, 1000, self.gamma)\n",
    "        self.model.setScheduler(scheduler, 0)\n",
    "\n",
    "    # Return cross entropy loss function.\n",
    "    def loss_function(self):\n",
    "        loss_function = nn.CrossEntropyLoss()\n",
    "        return loss_function\n",
    "    \n",
    "    # Trains the experiment.\n",
    "    # TODO: create proper methods for trainng each data set\n",
    "    def train(self):  \n",
    "        # Load the dataset into a data loader\n",
    "        data_set = fashion_MNIST_set(self.args)\n",
    "        # data_set = MNIST_set(self.args)\n",
    "        data_loader = DataLoader(data_set, batch_size=1, shuffle=True)\n",
    "        \n",
    "        # Train model on dataset\n",
    "        self.model.train()\n",
    "        \n",
    "        optimizer = self.optimizer()\n",
    "        if self.gamma !=0 : self.set_hebbian_scheduler()\n",
    "        \n",
    "        for _ in range(self.num_epochs):\n",
    "            for i, data in enumerate(data_loader):\n",
    "                inputs, labels=data\n",
    "                self.model(inputs, clamped_output=self.oneHotEncode(labels, 10))\n",
    "                optimizer.step()\n",
    "        \n",
    "            \n",
    "    # Given a tensor of labels, returns a one hot encoded tensor for each label.\n",
    "    def oneHotEncode(self, labels, num_classes):\n",
    "        one_hot_encoded = torch.zeros(len(labels), num_classes)\n",
    "        one_hot_encoded.scatter_(1, labels.unsqueeze(1), 1)\n",
    "    \n",
    "        return one_hot_encoded.squeeze()\n",
    "        \n",
    "    # Visualizes the weights associated with the first feature detector layer.\n",
    "    def visualizeWeights(self):\n",
    "        self.model.visualizeWeights()\n",
    "    \n",
    "    def test(self):\n",
    "        data_set=fashion_MNIST_set(self.args, 0)\n",
    "        data_loader=DataLoader(data_set, batch_size=1, shuffle=True)\n",
    "        cor=0\n",
    "        tot=0\n",
    "        for _, data in enumerate(data_loader):\n",
    "            inputs, labels=data\n",
    "            outputs = torch.argmax(self.model(inputs, None, train=0))\n",
    "            if outputs.item()==labels.item():\n",
    "                cor+=1\n",
    "            tot+=1\n",
    "        return cor/tot\n",
    "    \n",
    "    def printExponentialAverages(self):\n",
    "        A=torch.log(experiment.model.hebbian_layer.exponential_average).tolist()\n",
    "        plt.scatter(range(len(A)), A)\n",
    "        for i, (x, y) in enumerate(zip(range(len(A)), A)):\n",
    "            plt.text(x, y, f'{i}', ha='center', va='bottom')\n",
    "        plt.xlabel(\"Feature Selector\")\n",
    "        plt.ylabel(\"Log (Exponential Average)\")\n",
    "        plt.title(\"Logged Exponential Averages of Each Feature Selector\")\n",
    "        \n",
    "    def activeClassifierWeights(self, beta):\n",
    "        return self.model.classifier_layer.activeClassifierWeights(beta)\n",
    "\n",
    "\n",
    "# TODO: get rid of the code here and move it somewhere else\n",
    "if __name__==\"__main__\":\n",
    "    experiment=MLPExperiment(None, 784, 256 , 10, lamb=1, num_epochs=1, heb_lr=0.1)\n",
    "    experiment.train()\n",
    "    experiment.visualizeWeights(10, classifier=0)\n",
    "    experiment.visualizeWeights(10, classifier=1)\n",
    "    #experiment.test()\n",
    "    \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hebbian_network.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers.hebbian_layer import HebbianLayer\n",
    "import torch.nn as nn \n",
    "\n",
    "class HebbianNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Constructor method\n",
    "    @param\n",
    "        input_dimension (int) = number of inputs\n",
    "        hidden_layer_dimension (int) = number of neurons in hidden layer\n",
    "        output_dimension (int) = number of output neurons\n",
    "        heb_lr (float) = learning rate of NN\n",
    "        lamb (float) = hyperparameter for lateral neuron inhibition\n",
    "        eps (float) = small value to avoid 0 division\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dimension, hidden_layer_dimension, output_dimension, heb_lr=1, lamb=1, eps=10e-5):\n",
    "        super(HebbianNetwork, self).__init__()\n",
    "        self.input_dimension = input_dimension\n",
    "        self.output_dimension = output_dimension\n",
    "        self.hidden_layer_dimension = hidden_layer_dimension\n",
    "        self.hebbian_layer = HebbianLayer(self.input_dimension, self.hidden_layer_dimension, False, lamb=lamb, heb_lr=heb_lr, eps=eps)\n",
    "        self.classifier_layer = HebbianLayer(self.hidden_layer_dimension, self.output_dimension, True, lamb=lamb, heb_lr=heb_lr)\n",
    "    \n",
    "    \"\"\"\n",
    "    Method to set scheduler to either the classification layer or the hebbian layer\n",
    "    @param\n",
    "        scheduler (layers.Scheduler) = a scheduler\n",
    "        classifier (bool) = true if setting scheduler for classifier layer\n",
    "    \"\"\"\n",
    "    # TODO: modify this and make it more reusable -> might need to change other parts of the code (hebbian_layer.py)\n",
    "    def setScheduler(self, scheduler, classifier):\n",
    "        if classifier:\n",
    "            self.classifier_layer.setScheduler(scheduler)\n",
    "        else:\n",
    "            self.hebbian_layer.setScheduler(scheduler)\n",
    "\n",
    "    \"\"\"\n",
    "    Method that defines how an input data flows throw the network\n",
    "    @param\n",
    "        x (torch.Tensor) = input data as a tensor\n",
    "        clamped_out (???) = parameter to clamp the output #WTV this means\n",
    "        train (int) = true if in training\n",
    "    \"\"\"   \n",
    "    def forward(self, x, clamped_output=None, train=1):\n",
    "        x=self.hebbian_layer(x, clamped_output=None, train=train)\n",
    "        x=self.classifier_layer(x, clamped_output=clamped_output, train=train)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    # Method to visualize the weights/features learned by each neuron during training\n",
    "    def visualizeWeights(self):\n",
    "        self.hebbian_layer.visualizeWeights(classifier=0)\n",
    "        self.classifier_layer.visualizeWeights(classifier=1)\n",
    "\n",
    "# TODO: remove this from code\n",
    "if __name__==\"__main__\":\n",
    "    network = HebbianNetwork(784, 256, 10)\n",
    "    print(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scheduler.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Class to manage and update learning rate of optimization process\n",
    "\"\"\"\n",
    "class Scheduler():\n",
    "    \"\"\"\n",
    "    Contructor method\n",
    "    @param\n",
    "        heb_lr (float) = learing rate\n",
    "        step_size (int) = step size -> how often the learning rate should be updated\n",
    "        gamma (float) = decay factor -> factor to decay learning rate\n",
    "    @attr.\n",
    "        flag (str) = a string indicating wether the data is used for training or testing\n",
    "        data_frame (list-like object) = dataset for training/testing\n",
    "        labels (torch.Tensor) = labels of dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, heb_lr, step_size, gamma=0.99):\n",
    "        self.lr = heb_lr\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "        # epoch counter\n",
    "        self.epoch = 0\n",
    "\n",
    "    \"\"\"\n",
    "    Method used to increment epoch counter and update learning rate at every multiple of step size\n",
    "    \"\"\"    \n",
    "    def step(self):\n",
    "        self.epoch += 1\n",
    "        if (self.epoch % self.step_size) == 0:\n",
    "            self.lr *= self.gamma\n",
    "        return self.lr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
