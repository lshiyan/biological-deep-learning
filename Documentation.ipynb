{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # Creates argument parser object\n",
    "    parser = argparse.ArgumentParser(description='Biological deep learning')\n",
    "    \n",
    "    # Adds arguments to the parser and sets their default values\n",
    "    parser.add_argument('--is_training', type=int, default=1, help='status')\n",
    "    parser.add_argument('--data_root', type=str, default=\"data/mnist/\")\n",
    "    parser.add_argument('--train_data_filename', type=str, default=\"mnist_train.csv\")\n",
    "    parser.add_argument('--test_data_filename', type=str, default=\"mnist_test.csv\")\n",
    "    parser.add_argument('--learning_rate', type=int, default=0.001)\n",
    "    parser.add_argument('--num_epochs', type=int, default=100)\n",
    "    \n",
    "    # Parse arguments given to the parser\n",
    "    args=parser.parse_args()\n",
    "    \n",
    "    if args.is_training:\n",
    "        print(\"training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data_loader.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\"\n",
    "Class to setup datasets (seperating images and labels) for both training and testing purposes\n",
    "\"\"\"\n",
    "class Image_Data_Set(Dataset):\n",
    "    \"\"\"\n",
    "    Contructor method\n",
    "    @param\n",
    "        train (bool) = is the dataset in training or testing\n",
    "        name (str) = name of data set\n",
    "    @attr.\n",
    "        flag (str) = a string indicating wether the data is used for training or testing\n",
    "        data_frame (list-like object) = dataset for training/testing\n",
    "        labels (torch.Tensor) = labels of dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, train=True, name):\n",
    "        self.__name = name\n",
    "        self.__flag = 'train' if train else 'test'\n",
    "        self.__data_frame = None\n",
    "        self.__labels = None\n",
    "\n",
    "    \"\"\"\n",
    "    Set up the data frame and labels with the defined data set\n",
    "    @param\n",
    "        data_set (str) = string defining path to .csv file of data set\n",
    "    \"\"\"\n",
    "    def setup_data(self, data_set):\n",
    "        self.data_frame=pd.read_csv(data_set, header=None)\n",
    "        self.labels=torch.tensor(self.dataframe[0].values)\n",
    "        self.data_frame=torch.tensor(self.data_frame.drop(self.data_frame.columns[0], axis=1).values, dtype=torch.float)\n",
    "        self.data_frame/=255\n",
    "    \n",
    "    # Normalize number(s) assuming max value = 255\n",
    "    def normalize(self, row):\n",
    "        row=row/255\n",
    "    \n",
    "    # Get length of data frame    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    # Attritube getter functions\n",
    "    def get_flag(self):\n",
    "        return self.__flag\n",
    "    \n",
    "    def get_name(self):\n",
    "        return self.__name\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        label = self.labels[idx]\n",
    "        features = self.dataframe[idx]\n",
    "        \n",
    "        return features, label\n",
    "\n",
    "\n",
    "\n",
    "# TODO: Write this part of the code somewhere else\n",
    "if __name__==\"__main__\":\n",
    "    #mnist=MNIST_set(None)\n",
    "    #fashion_MNIST_set(None)\n",
    "    mnist = Image_Data_Set(name=\"MNIST\")\n",
    "    fashion_mnist = Image_Data_Set(name=\"Fashion_MNIST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mnist_factory.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\"\"\"\n",
    "Converts image file and labels into .csv file\n",
    "@param \n",
    "    img_file (str) = relative path to image file\n",
    "    label_file (str) = relative path to label file\n",
    "    out_file (str) = relative path to out file\n",
    "    data_size (int) = number of data inputs to read\n",
    "\"\"\"\n",
    "\n",
    "def convert(img_file, label_file, out_file, data_size):\n",
    "    # Get absolute path of all the necessary files\n",
    "    project_root = os.getcwd()\n",
    "    img_file = os.path.join(project_root, img_file)\n",
    "    label_file = os.path.join(project_root, label_file)\n",
    "    out_file = os.path.join(project_root, out_file)\n",
    "\n",
    "    # Open all necessary files\n",
    "    imgs = open(img_file, \"rb\")\n",
    "    out = open(out_file, \"w\")\n",
    "    labels = open(label_file, \"rb\")\n",
    "    \n",
    "    # Skip start of file because???\n",
    "    # TODO: why skip bytes?\n",
    "    imgs.read(16)\n",
    "    labels.read(8)\n",
    "    \n",
    "    # Create a 2D list of images where each image is a 1D list where the first element is the label\n",
    "    img_size = 28*28\n",
    "    images = []\n",
    "\n",
    "    for i in range(data_size):\n",
    "        image = [ord(labels.read(1))]\n",
    "        for j in range(img_size):\n",
    "            image.append(ord(imgs.read(1)))\n",
    "        images.append(image)\n",
    "\n",
    "    # Convert each image from 1D list to a comma seperated str and write it into out file\n",
    "    for image in images:\n",
    "        out.write(\",\".join(str(pix) for pix in image)+\"\\n\")\n",
    "    \n",
    "    # Close files\n",
    "    imgs.close()\n",
    "    out.close()\n",
    "    labels.close()\n",
    "\n",
    "# TODO: These lines should be written somewhere else\n",
    "convert(\"data/mnist/train-images.idx3-ubyte\", \"data/mnist/train-labels.idx1-ubyte\",\n",
    "        \"data/mnist/mnist_train.csv\", 60000)\n",
    "convert(\"data/mnist/t10k-images.idx3-ubyte\", \"data/mnist/t10k-labels.idx1-ubyte\",\n",
    "        \"data/mnist/mnist_test.csv\", 10000)\n",
    "\n",
    "convert(\"data/fashion_mnist/train-images.idx3-ubyte\", \"data/fashion_mnist/train-labels.idx1-ubyte\",\n",
    "        \"data/fashion_mnist/fashion-mnist_train.csv\", 60000)\n",
    "convert(\"data/fashion_mnist/t10k-images.idx3-ubyte\", \"data/fashion_mnist/t10k-labels.idx1-ubyte\",\n",
    "        \"data/fashion_mnist/fashion-mnist_test.csv\", 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mlp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from data.data_loader import MNIST_set, fashion_MNIST_set\n",
    "from models.hebbian_network import HebbianNetwork\n",
    "from layers.scheduler import Scheduler\n",
    "\n",
    "\n",
    "class MLPExperiment():\n",
    "    \n",
    "    def __init__(self, args, input_dimension, hidden_layer_dimension, output_dimension, \n",
    "                 lamb=1, heb_lr=1, grad_lr=0.001, num_epochs=3, gamma=0, eps=10e-5):\n",
    "        self.model=HebbianNetwork(input_dimension, hidden_layer_dimension, \n",
    "                                  output_dimension, heb_lr=heb_lr, lamb=lamb, eps=eps)#TODO: For some reason my hebbian network is not processing batches together.\n",
    "        self.args=args\n",
    "        self.num_epochs=num_epochs\n",
    "        self.grad_lr=grad_lr \n",
    "        self.heb_lr=heb_lr\n",
    "        self.gamma=gamma\n",
    "        \n",
    "    #Returns ADAM optimize for gradient descent.\n",
    "    def optimizer(self):\n",
    "        optimizer = optim.Adam(self.model.parameters(), self.grad_lr)\n",
    "        return optimizer\n",
    "    \n",
    "    #Sets the scheduler for the feature detector layer of our network.\n",
    "    def set_hebbian_scheduler(self):\n",
    "        scheduler=Scheduler(self.heb_lr, 1000, self.gamma)\n",
    "        self.model.setScheduler(scheduler, 0)\n",
    "\n",
    "    #Returns cross entropy loss function.\n",
    "    def loss_function(self):\n",
    "        loss_function = nn.CrossEntropyLoss()\n",
    "        return loss_function\n",
    "    \n",
    "    #Trains the experiment.\n",
    "    def train(self):  \n",
    "        data_set=fashion_MNIST_set(self.args)\n",
    "        # data_set=MNIST_set(self.args)\n",
    "        data_loader=DataLoader(data_set, batch_size=1, shuffle=True)\n",
    "        \n",
    "        self.model.train()\n",
    "        \n",
    "        optimizer=self.optimizer()\n",
    "        if self.gamma !=0 : self.set_hebbian_scheduler()\n",
    "        \n",
    "        for _ in range(self.num_epochs):\n",
    "            for i, data in enumerate(data_loader):\n",
    "                inputs, labels=data\n",
    "                self.model(inputs, clamped_output=self.oneHotEncode(labels, 10))\n",
    "                optimizer.step()\n",
    "        \n",
    "            \n",
    "    #Given a tensor of labels, returns a one hot encoded tensor for each label.\n",
    "    def oneHotEncode(self, labels, num_classes):\n",
    "        one_hot_encoded = torch.zeros(len(labels), num_classes)\n",
    "        one_hot_encoded.scatter_(1, labels.unsqueeze(1), 1)\n",
    "    \n",
    "        return one_hot_encoded.squeeze()\n",
    "        \n",
    "    #Visualizes the weights associated with the first feature detector layer.\n",
    "    def visualizeWeights(self):\n",
    "        self.model.visualizeWeights()\n",
    "    \n",
    "    def test(self):\n",
    "        data_set=fashion_MNIST_set(self.args, 0)\n",
    "        data_loader=DataLoader(data_set, batch_size=1, shuffle=True)\n",
    "        cor=0\n",
    "        tot=0\n",
    "        for _, data in enumerate(data_loader):\n",
    "            inputs, labels=data\n",
    "            outputs = torch.argmax(self.model(inputs, None, train=0))\n",
    "            if outputs.item()==labels.item():\n",
    "                cor+=1\n",
    "            tot+=1\n",
    "        return cor/tot\n",
    "    \n",
    "    def printExponentialAverages(self):\n",
    "        A=torch.log(experiment.model.hebbian_layer.exponential_average).tolist()\n",
    "        plt.scatter(range(len(A)), A)\n",
    "        for i, (x, y) in enumerate(zip(range(len(A)), A)):\n",
    "            plt.text(x, y, f'{i}', ha='center', va='bottom')\n",
    "        plt.xlabel(\"Feature Selector\")\n",
    "        plt.ylabel(\"Log (Exponential Average)\")\n",
    "        plt.title(\"Logged Exponential Averages of Each Feature Selector\")\n",
    "        \n",
    "    def activeClassifierWeights(self, beta):\n",
    "        return self.model.classifier_layer.activeClassifierWeights(beta)\n",
    "if __name__==\"__main__\":\n",
    "    experiment=MLPExperiment(None, 784, 256 , 10, lamb=1, num_epochs=1, heb_lr=0.1)\n",
    "    experiment.train()\n",
    "    experiment.visualizeWeights(10, classifier=0)\n",
    "    experiment.visualizeWeights(10, classifier=1)\n",
    "    #experiment.test()\n",
    "    \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hebbian_network.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers.hebbian_layer import HebbianLayer\n",
    "import torch.nn as nn \n",
    "\n",
    "class HebbianNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dimension, hidden_layer_dimension, output_dimension, heb_lr=1, lamb=1, eps=10e-5):\n",
    "        super(HebbianNetwork, self).__init__()\n",
    "        self.input_dimension=input_dimension\n",
    "        self.output_dimension=output_dimension\n",
    "        self.hidden_layer_dimension=hidden_layer_dimension\n",
    "        self.hebbian_layer=HebbianLayer(self.input_dimension, self.hidden_layer_dimension, False, lamb=lamb, heb_lr=heb_lr, eps=eps)\n",
    "        self.classifier_layer=HebbianLayer(self.hidden_layer_dimension, self.output_dimension, True, lamb=lamb, \n",
    "                                           heb_lr=heb_lr)\n",
    "    \n",
    "    def setScheduler(self, scheduler, classifier):\n",
    "        if classifier:\n",
    "            self.classifier_layer.setScheduler(scheduler)\n",
    "        else:\n",
    "            self.hebbian_layer.setScheduler(scheduler)\n",
    "            \n",
    "    def forward(self, x, clamped_output=None, train=1):\n",
    "        x=self.hebbian_layer(x, clamped_output=None, train=train)\n",
    "        x=self.classifier_layer(x, clamped_output=clamped_output, train=train)\n",
    "        return x\n",
    "    \n",
    "    def visualizeWeights(self):\n",
    "        self.hebbian_layer.visualizeWeights(classifier=0)\n",
    "        self.classifier_layer.visualizeWeights(classifier=1)\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    network=HebbianNetwork(784, 256, 10)\n",
    "    print(network)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
